\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{comment}
\newcommand{\om}{\omega_n}
\title{Quantum Chemistry}
\author{Eesh Gupta }
\date{January 6, 2020}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Aims}
Purpose: Given an intuitive outlook on quantum computation of molecular
energies.

This paper is not rigorous. There is too much material. Nonetheless, I
will present equations and wierd constants to aid intuition of certain
methods.

\section{Introduction}
This paper concerns with understanding the recent advancements made in
simulation of electronic structure through quantum computers. A good
question to ask is why simulate molecules? Why bother about calculating
the ground state energies of molecules?

Suppose you are the manager of a fertilizer making company. You look at the
bills and see a huge chunk of your money being invested in carrying out the
born haber process. To save some money, you look more into this issue and
find out that expense is largely coming from the maintaining high pressures
and temperatures to carry out the reaction. So the obvious solution is to
tell your scientists to somehow make this reaction happen without the high
pressures and temperatures. But the scientists complain that we need those
conditions to allow the action of a really important enzyme called
nitrogenase. Well you ask them to use their science to study the enzyme and
somehow make it work to reduce costs. But the scientists say their science
isn't advanced enough because the analysis of electronic structure requires
exponential costs. And you are left with the high costs and you can't get
your next promotion.

Accurately calculating the ground state energies of small molecules such as
hydrogen gas or lithium hydride might seem insignificant in comparison to
simulations of large drug molecules that are of interest to practical
applications. However, not all aspects of simulations of such drug molecules
 are important. If we decompose such problem in a way that the most important
 aspect of the system is accurately simulated and other aspects are simulated
 potentially in a less accurate manner, we can see why our work on small
 molecules may be of importance to solve the much larger simulation problems.


\section{Chemistry Context}

What exactly is the electronic structure problem? What we need to find is the
minimum energy configuration of an atom or the ground state energy of a molecule.
To study the energy profile of a molecule, we need to be familiar
with the term potential energy surface. Abbreviated as PES, this term refers
to a graph of energy based on a set of parameters like bond angle, bond length,
etc. Making this plot requires you to calculate energy for every possible
configuration of those parameters. It's easy to see that the more the parameters,
the harder (in terms of computational cost) it becomes to make these plots.

But how do we even compute the energy? Well we gotta use something called the
Hamiltonian \(H\). When the hamiltonian acts on the exact ground state
wavefunction \(\Psi_{0}\), the eigenvalue gives us the exact ground state
\(E_0\) according to the equation:
\[H\Psi_{0} = E_0\Psi_{0}\]

\subsection{Brief Review of Quantum Mechanics}

\subsubsection{State}

The state of the system is completely specified by \(\Psi(r, z)\) where
\(r\) denotes the position of system and \(z\) denotes spin. Think system
here as an electron.

Wavefunction: Reports the ``status'' of a particle in terms of position and
time. If we know the wavefunction \(\Psi(r, t)\), we can find all the
measurable properties of a particle

Now wavefunction is a complex number but if you square it, you'll get the
real part only and this real part can be interpreted physically.

Born Interpretation:
\(\Psi*(x)\Psi(x) dx = \) Probability of a particle being in the space
between x and dx, AS SHOWN IN FIGURE ! This is why we also call the square
of the norm of the wavefunction proabability density function...because
it tells us the probability that the particle is in some space.

Bonus Intuition: What do we mean when we say that the wavefunction of a
particle is normalized? Well, if it is , you get
\[\int_{-\infty}^{\infty}\Psi*(x)(\Psi(x) dx = 1 \]
which makes sense since the probability of a particle being in all of
universe is 1.

\subsubsection{Operators}
In quantum physics, to measure a physical observable like position, momentum
or energy of a particle, you need a hermitian, linear operator or what
John Wheeler called machines. They take an input vector and give you an output
vector.

Operator: Does something to a state to output a new state (a matrix
          transformation in the Hilbert Space?!?)

Now what the heck does hermitian mean? Well it means that the operator/matrix
must have real eigenvalues. In the next postulate you will learn that the
eigenvalues correspond to measurement. And you do want a real number measurement,
don't you?!?
\subsubsection{Eigenvalue}

Measuring an observable on some state is equivalent to computing the eigenvalue
of the eigenfunctions of the observable?!?

\subsubsection{Expectation Value}

Expectation values is the weighted sum of eigenvalues of an operator. For example,
the expectation value of the pauli matrices is 0 because all their eigenvalues
consist of +1 and -1.
\subsection{Second Quantization}
Suppose a hypothetical atom has the orbitals or discrete energy levels indexed
as following \({A, B, C , D...}\). Also suppose the system
only contains one electron in the state
\[\ket{\Psi}_1 = \ket{A}_1 + \ket{B}_1\]
where electron 1 is simultaneously occuping orbitals A and B. So far, our
notation looks elegant and easy to interpret. But when we account for the
second electron, let's say in the state \(\ket{\Psi}_2 = \ket{C}_2 \) occupying
orbital C, our notation for the state of whole system \(\ket{\Phi}\) gets
messy. You might assume it will look like this:
\[\ket{\Phi} = (\ket{A}_1 + \ket{B}_1)\otimes\ket{C}_2\]
But, this is wrong since we have failed to account for exchange symmetry. What
is exchange symmetry? In a quantum mechanical systems, particles are
undistinguishable. For example, in the equation \(1 + 1 = 2\) the two 1's are
same. (Cite stack exchange answer) If we were to swap the 1 on the right side of \(+\) with the 1 on the left
side of \(+\), no one would notice any difference. (Start citing quora answer)
Similarly if an observer is looking at a quantum system and observes some
probability density \(P(1,2)\), if we swap the electrons 1 and 2, the probability
density should stay the same.
\[P(1,2) = P(2,1)\]
Equivalently,
\[|\ket{\Psi(1,2)}|^2 = |\ket{Psi(2,1)}|^2\]
But when we unsquare both sides, we have to attach a phase as shown below
\[\ket{\Psi(1,2)} =e^(i\Phi)\ket{Psi(2,1)}\]
where \(0<=\Phi<=2\pi\). `It turns out that all the fundamental particles
correspond to either \(\Phi=0\), called bosons, or \(\Phi=\pi\),
called fermions.'' Hence we obtain the antisymmetry principle for fermions
which is
\[\ket{Psi(1,2)} = -\ket{Psi(2,1)}\]
%https://www.quora.com/Why-do-fermions-have-anti-symmetric-wave-functions Daniel Merthe
% https://physics.stackexchange.com/questions/122570/what-is-more-fundamental-fields-or-particles Wet Savanna Animal
Coming back to our case, the combined state of the system will then be
\[\ket{\Phi} = (\ket{A}_1 + \ket{B}_1)\otimes\ket{C}_2 -
\ket{C}_1\otimes(\ket{A}_2 + \ket{B}_2)\]

which can be represented as a determinant of a matrix as shown below:

\det\begin{pmatrix}
\(\ket{A}_1 + \ket{B}_1\) & \(\ket{C}_1\) \\ \(\ket{A}_2 + \ket{B}_2\) & \(\ket{C}_2\)
\end{pmatrix}

As you can tell, the more orbitals we have and the more electrons we have,
the worse-looking such notation becomes. Here we introduce second quantization
through an analogy to violin strings.

Suppose the state of the some system is a violin string, doing some crazy
oscillations. To represent that wave, using the fourier series formulas, we
deconstruct the complicated wave into sum of simpler waves called vibrational
modes. Now each vibrational mode does not contribute equally to the
complicated wave. Some modes contribute more than others, represented by
the weight or amplitude of the mode \(a_n\). So, we have
\[\text{complicated wave} = \sum_{\text{mode n}}^{\infty} a_n * \text{mode}_n\]

Insert GRAPHIC of wave decomposition.

In this analogy, each mode represents a particular orbital or state and the
amplitude refers to the number of particles occupying that state. Notice how
in this formulation, we are summing all the modes and not the amplitudes.
Each mode \(\text{mode}_n\) or orbital is assigned an amplitude \(a_n\) or
number of electrons. The notation is clean and makes sense.

But if we were to sum over the particles or unit amplitudes, suddenly we lose
intuition. For example, suppose a kth mode \(\text{mode}_k\) has the amplitude
\(a_k = 3\). Since \(3 = 1 + 1 + 1\), each 1 or unit amplitude
corresponds to a particle. Each of these particles is assigned the same orbital
or mode \(\text{mode}_k\). So far so good.
If we have lets say the \(\text{mode}_(k+1)\) with \(a_{k+1} = 1\), what makes
the electron or unit amplitude in \(\text{mode}_{k+1}\) different from the
first electron or the first unit amplitude in \(\text{mode}_k\)? Nothing. So
now we have to account for exchange symmetry and antisymmetry. More specifically,
we would have to account for the state where the first electron of
\(\text{mode}_k\) is swapped with the electron in \(\text{mode}_(k+1)\). And now
representing the combined state of the system by summing over all particles
will be a nightmare.

And moreover, this method doesn't even make sense. By summing over all the
particles, we are summing over all unit amplitudes. But amplitude is just a number
not some entity. More appropeiate way is to consider the mode as an entity and
amplitudes or the number of particles is that mode's weight/contribution to
the complicated wave or the combined state.

So the analogy illustrates that it is more intuitive to emphasize how excited
each state or orbital is instead of showing what electron occupies what orbital.
In fact, assigning numbers to electrons doesn't make sense because they are
indistinguishable. And because we did so, we ended up with the horrible notation
as a result of the antisymmetry principle. To curb this mess, we can try to
avoid assigning numbers or names to electrons and just focus on showing
how excited each orbital configuration is.

 To show the underlying simplicity of the approach shown in the analogy,
we turn back to our example from before.
Note that each state will now be represented as
\[\ket{n_a}_A\ket{n_b}_B\ket{n_c}_C\ket{n_d}_D\]
where each \(n_i\) denotes the number of particles in the corresponding orbital.
So from previous discussion, \(\Psi_1\) becomes
\[\ket{\Psi_1} = \ket{1}_A\ket{0}_B\ket{0}_C\ket{0}_D +
\ket{0}_A\ket{1}_B\ket{0}_C\ket{0}_D\]
or for sake of simplicity
\[\ket{\Psi_1} = \ket{1000} + \ket{0100}\]
and \(\Psi_2\) becomes
\[\ket{\Psi_2} = \ket{0}_A\ket{0}_B\ket{1}_C\ket{0}_D\]
or
\[\ket{\Psi_2} = \ket{0010}\]
Hence the combined state is
\[\ket{\Phi} = (\ket{1}_A\ket{0}_B\ket{0}_C\ket{0}_D + \ket{0}_A\ket{1}_B\ket{0}_C\ket{0}_D)\otimes(\ket{0}_A\ket{0}_B\ket{1}_C\ket{0}_D) \]
which is equivalent to
\[\ket{\Phi} = (\ket{1000} + \ket{0100})\otimes(\ket{0010})\]
\[\ket{\Phi} = \ket{1010} + \ket{0110}\]
Notice that \(\ket{1010}\) says that 2 electrons occupy orbital A and orbital C.
It doesn't matter which electron occupies which because we haven't labeled
or assigned an id to each electron. If you were to id each electron as we did
previously, then you would have to account for antisymmetry. In other words,
 in first quantization, we had to explicitly account for
indistinguishable particles which made our notation cumbersome. But in
second quantization, that notion of exchange symmetry is already embedded into
our notation so there is no need to add further details

If particles are reduced to units of excitation under second quantization,
what are they excitations of? To explain this, let's turn back to our
violin example. Recall that the violin string made some complicated
wave and we used fourier analysis to deconstruct that wave into simpler
plane waves. In this analogy, the violin string represents a field and
those simpler plane waves/modes represent quantum states. Each particle
is a unit excitation of some mode in that field. For example, in an ideal
hydrogen atom, ignoring the nucleus, the whole atom is a field and the only
mode or quantum state contributing to that field/complicated wave is the 1s
orbital. Moreover, that 1s orbital would have an amplitude of 1 since that
represents a unit excitation in 1s orbital which is the sole electron in
1s orbital. In an ideal lithium atom, we see more quantum states with addition of the
2s orbital. The 1s orbital will now have 2 units of excitation and the 2s orbital
will have a single unit, giving us a total of 3 electrons.

At this point we will introduce creation and annhilation operators. Suppose
you notice that the kth orbital of some atom/molecule is unoccupied and you want
to add an electron to that orbital. Right now, the state is
\(\ket{0}\) (vaccum state). So what you do is open your second quantization
toolkit and pull out the creation operator \(a^{\dagger}_k\) and apply to the
vaccum state as shown:
\[\ket{k} = a^{\dagger}_k\ket{0}\]
By applying that creation operator, you have changed the status of the kth
orbital from being unoccupied (being vaccum state) to occupied. To undo this
change, we use the annhilation operator as following:
\[\ket{0} = a_k\ket{k}\]
We will use these tools to construct the hamiltonian for the system. Before we
dive into the hamiltonian jungle, its important to clarify the nature of
orbitals. In the conventional sense, we refer to orbitals as a region within
a subshell occupied by 2 electrons with opposite spins. From now on, these
orbitals will be referred to as spatial orbitals. These spatial orbitals will
each contain 2 spin orbitals which will only occupy 1 electron. This will make
representing spin orbitals in terms of quantum states easier as
\[\ket{0} \implies \text{unoccupied}\]
\[\ket{1} \implies \text{occupied}\]
This allows us to use binary representation where each digit corresponds to
an orbital. This is utilized in quantum fourier transform and quantum phase
estimation algorithms.
\subsection{Hartree Fock Theory}

To find the total energy of the system, we have to consider 4 factors:
\begin{enumerate}
  \item Electron-Nucleus attraction (Potential Energy)
  \item Electron-Electron repulsion (Potential Energy)
  \item Nucleon-Nucleon repulsion (Potential Energy)
  \item Kinetic energy of electrons
\end{enumerate}
Since the first and the fourth factors involve only one electron, we group
them to construct the following one-electron integral \(h_{pq}\)
\[h_{pq} = \]
The second factor involves 2 electrons and hence the corresponding two
electron integral is
\[h_{pqrs} =\]
The third factor requires no electrons so we we denote it as \(h_{nuc}\)
\[h_{nuc} =\]
%%%copy from mccleans paper on streategies for qc mol energies using ucc ansatz
Note that when we talk about simulating molecules, we use the term ``electronic
structure'' more often as opposed to ``molecular structure''. This is because
when we zoom down to level of atoms, nucleons are bigger than electrons by
4 orders of magnitude. Hence, we utilize the Born-Oppenheimer approximation,
according to which electrons are treated as quantum particles but nuclei are
treated as classical particles, fixed in their positions with no kinetic
energies. This simplifies the hamiltonian as \(h_{nuc}\) is treated as a constant.
With all these

%To show uncoupled nature of equations, credit TMP CHEM
To demonstrate the Hartree fock method, we will use an example of the helium
atom. Note that helium atom has 2 electrons.
The hamiltonian of this system looks something like this:
\[H = \text{Kinetic Enegies of electrons} + \text{Electron-Nucleon Repulsions} +
\text{Electron-Electron Repulsions}\]
In the first 2 terms, we only work with 1 electron at a time. Computationally,
this translates to single-electron integrals with 3 degrees of freedom (because
the electrons live in a 3 dimensional space). However, the last term involves
2 electrons. Computationallu, this tranlates to 2-electron integrals with
6 degrees of freedom ( 3 for each electron). Since each additional degree
of freedom is computationally expensive, the last term becomes significantly
harder to compute.
To avert this mess, Hartree-fock suggested we assign an effective hamiltonian
to each electron and introduce mean field operator. The resulting hamiltonians
look like this:
\[H_1 = \text{Kinetic Enegies of electron 1} + \text{Electron 1-Nucleon Repulsion} +
V(r_1)\]
\[H_2 = \text{Kinetic Enegies of electron 2} + \text{Electron 2-Nucleon Repulsion} +
V(r_2)\]
What is this mean field operator\(V(r)\)?
In Hartree Fock method, we only want to evaluate the single electron integral
and not the 2 electron integral.

///Brief Explanation of Hartree Fock Theory
%%% https://www.quora.com/What-is-an-intuitive-explanation-of-the-Hartree-Fock-method

To explain Hartree Fock, we will rely on a high level description analogy by Chemical
Physicist Jay Whitfield.

Here is a brief sketch of problem

Problem: Need to compute orbitals of electrons in some atom/molecule.

To do so, you need to evaluate
Problem Rephrased:

Solution:

Context of Analogy: Orchestra Players need to tune to a reference note say F.
Some players are too sharp. SOme players instruments are too flat. No-body
has any clue what the real note is.

\subsubsection{Initial Guess}
The orchestra players start playing their own variation of F.

We pick a certain set of atomic orbitals which may or may not be the true
orbitals of the atom.
\subsubsection{Calculation of Mean Field}
Once everyone starts playing, the players hear the combined note.

We use these orbitals to calculate mean field for each electron in our atomic
system. When we have done so, we can start calculating the effective hamiltonians
for each of our electrons.
\subsubsection{Obtaining New Orbitals}
If the player's individual note is sharper than the combined note, then he/she
will make the necessary adjustments to make their note flatter. In this sense,
all players are trying to converge their sounds to the combined note they heard
before.

We use a set of equations to calculate the new orbitals using the hamiltonians
obtained from the previous step. Note that ideally orbitals should be
eigenstates of their respective hamiltonians. So we should not obtain new
orbitals. But if we do obtain new orbitals, then there must be something wrong
with our initial orbitals (the ones we guessed and used to calculate the mean
field operators). This implies that our mean field operator is also wrong.

OBVIOUS HOLE: If the mean field operator is wrong, how do we know that the
new orbitals we obtained are any better than the new orbitals? How do
we know that the resultant orbitals are improvement upon the original
orbitals? How do we know that SCF converges?

\subsubsection{Repeat Process}
The players play the modified note and hear the new combined note. The above
steps are repeated until every player is in tune with the final combines note.

We keep generationg mean opoerators until the orbitals don't change when acted
upon by their hamiltonians. At that point those orbitals won't generate a new
mean field. They will keep generating the same mean field..ie the mean field
becomes self consistent.


\subsection{Post Hartree Fock Methods}
\subsubsection{Basis Sets}
A basis set is comprised of a set of known functions whose linear combo produces an approximation for a molecular orbital (unknown function)
Basis Functions = Atomic Orbitals
Think of an MO as a vector in a coord space with inifinitely many directions. The same coord space is spanned by basis functions. So when you choose a finite basis set, you restrict yourself to calculate MO along those finite directions...and hence obtaining an approximation.
Size
If basis set is complete (ie there are infinite basis functions), then their combo exactly describes an MO
Type
The better a single basis function is able to produce an unknown function, the less basis functions we need to approach some re accuracy.
Why Quantum (Maybe)
Classical computers cannot work with infinite basis functions...we can’t get good accuracy on them
Other factors such as integrating over basis function is computationally expensive.
2 types of Basis Functions
Slater Type Orbitals
Gaussian Type Orbitals - improvement over STOs
Difference
GTOs have zero slope near nucleus; thus problems in describing electron behaviour near nucleus. StOS just have a cusp near nucleus
GTOs fall too rapidly away from nucleus
Thus, need more GTOs than STOs to achieve a given level of accuracy (ie to represent each STO, need a lin combo of GTOs)
Construction of Basis Sets
Terminology
A basis function (contracted function) is made of a linear combination of GTOs (primitive function)
It contains parameters such as the number of GTOs, the exponents on those GTOs and the coefficients of GTOs
Parameters
Exponents of GTOs
Determine the radial distance ( large= tight functions near nucleus; small = better approx tail behavior of wavefunction)
Can be determined by variational principle ( parameters that give lowest energy are the best)
The more the GTOs, the more the exponents that need to be optimized, the more the computational cost.

\subsubsection{Need for Post Hartree FOck Methods}
Molecular Orbitals (MOs) are made from basis functions which are atomic orbitals.
Now Hartree Fock only cares about the ground state MOs which are occupied. The unoccupied virtual MOs are ignored.
Thus, HF Slater determinant is formed out of the occupied MOs only.
But if we replace the occupied MOs in HF determinant with unoccupied virtual MOs (to represent some form of excitations), we can obtain excited determinants.
The number of virtual MOs is dictated by number of basis functions: the larger the basis set, the more the virtual MOs, the more excitations accounted for, the better the approximation for energy
Hence, for infinite basis, we recover all the correlation (previously ignored by HF) and therefore Schrodinger equation is solved exactly ( we obtain the exact energy eigenvalues)

The single slater determinant means HF ignores correlation

\subsection{Unitary Coupled Cluster}
Once you have the determinants for the many electron wavefunction, its time to determine the coefficients a’s as shown in the equation below:

Coupled Cluster
Exponential operator
What do they mean by connected and disconnected types? What is the physical interpretation of product of excitations? Why does CI include that and CC doesn’t?
Calculation of amplitudes
Variational Principle: Choosing a reference wavefunction, by either variational principle or some other method, and then using variational principle again to obtain all the amplitudes (computationally expensive as we are accounting for near infinite excited states)
Projecting the SE: Why are we using \(e^T\) and Ecc to calculate amplitudes when we use amplitudes to calculate those quantities? WHy is H made from single and doubly occupied shells?
2 confusing assumption in CC
Hamiltonian only contains interaction for 2 particles so we can ignore triply and quadruply excited states
That’s because triply and higher excited states require more than 2 electrons which is not relevant because Hamiltonian only deals with 2 body systems?!?
Brillouin’s Theorem
Approximation
Coefficient on quadruply excited states as product of coeffiencients on doubly excited states
KEY: double excited states...is accounting for any pair of electrons out of N electrons and accounting for all the excited-orbital-configuration
KEY: quadruply excited states is taking 4 electrons out of N electrons…..which is 2 pairs….so the permutations of exciting 4 electrons is approximately equal to product of permutations of doubly excited states ( remember 2 pairs)

Unitary Coupled cluster is just adding Tdagger on top of the exponential
operator. This makes operator unitarizes which makes it workable using
the variational principle(referenced later)

The UCCSD method, we only look at first and second excitations. If system is
strongly correlated or the higher excited states are equivalent in energy to
ground states, this method fails massively. BUT for smaller molecules, this
serves as a good approximation.
%% refeerence strategies for QC mol energies using UCC ansatz

\section{Quantum Computing}
What do QUantum computers have to offer here? Well calculation of mol. ground
state energies requires us to repeatedly create a state and calculate
expectation value of that parameterized state w.r.t hamiltonian. these tasks
can be efficiently performed on a quantum device.

To further see the correspondence between these methods, we examing quantum
encoding methods, which is telling us how to represent teh system using qubits.

\subsection{Quantum Encoding Methods}
Jordan WIgner: Every orbital can be described by a qubit. We already discussed
how 0 corr to unoccupied and 1 corr to occupied.
Remember those creation and annhilation operator. Turns out  if 0 is [1 0 ] and
1 is [0  1],
then we can write as :- formulas for creat/annhi

***We can talk about the anticommuting problem as well***

Parity Basis: 1 is occupied and 0 is empty ...qubit i stores sum modulo 2 of
occupation of all qubits less than or equal to i ....so only a single qubit
ops is applied to each qubit. But if you change qubit j, now you gotta change
all qubits of index less than j . so still linear time

Comparison between Parity and JORDAN: occupation # is local but parity is
nonlocal.
for parity tho, occupation number is nonlocal and parity is local.

Now a faster encoding paradigm exists called bravyi kitaev where every electronic
operation only requires O(logn) time...we achieve that by combining occupation
number and parity. How? Even qubits store only their occupation, off qubits
store their occupation + other qubits occupation number mod 2 (parity)

WHAT DO THESE METHODS TELL US:
We can encode orbitals into qubits and express creation and annihilation
operators as combo of pauli gates. Since creat and annhi ops make us hamiltonian,
we see that we can simulate hamiltonian processes using quantum circuits.
\subsection{Variational Method}

If \(\Psi_0\) denotes the exact ground state wavefunction and \(E_0\) denotes
the exact ground state energy, then we get this nice eigenvalue equation.

Now the unknowns in this equations are the exact ground state wavefunction
and the exact ground state energy.
While we could just find the eigenvalues of the hamiltonian, we don't know
the relative wight of those eigenvalues to give us the expectation of energy.

So what we do is we guess the wavefunction and calculate ground state energies
accordingly to this equation.

Now this approximate ground state energy will always be an upper bound to the
total ground state energy. Variational principle relies on make sure to
choose parameters in a way that reach the min energy possible. Then the
guess state is really close to exact ground state energy.

Problems with this description: What if we find the smallest eigenvalue and say
that the state is exactly that associated eigenstate...no need for this
iterative procedure.

\subsection{Quantum Computing Tools}

Universal feature of quantum algorithms:
\begin{enumerate}
  \item Initial state must be close to ground state
  \item Coherently (without loss of info) implement time evolution under
  Hamiltonian
\end{enumerate}
Why not QPE: too many gates and large fault taolerance?!? impossible for
NISQ.
%%%https://arxiv.org/pdf/1808.10402.pdf

Empowered with these tools, we can now describe the general approach behind the
variational quantum algorithms
\begin{enumerate}
  \item Initialize state
  \item Apply a series of paramertiezed gates to get ansatz
  \item obtain trial wavefunction
  \item measure expectation value of molecular hamiltonian ?!?
  \item Classical optimization routine ---> output new param and prepare new
  trial state which is ideally lower in energy
\end{enumerate}

***Discussion on hardware efficient and chemically efficient ansatz looks
unecessary.

\section{Variational Quantum Eigensolver}
\subsection{State Preparation}
The closer the initial state is to exact ground state, the more likely it is
that there will be convergence and variational method is successful. That's
why for our ansatz, we use the hartree fock which is a good starting point
for uncorrelated systems.

there are other types of ansatz if you are concerned with strongly correlated
systems. bc hf then does not have a lot of overlap with exact ground state
wavefunction.
\subsection{Energy Measurement}
AFter state  has been prepared, we move on to energy measurement which boils
down to measuring expectation value of each term and combing according to
the equation:
%%% hamiltonian averaging procedure strategies for ucc paper.
Now  we first have to explain how cluster operators can be represented by
simple pauli matrices. Then show how exponentiation of pauli happens
and then we can arrive at measuring expectation value of bunch of paulis.
\subsection{Parameter Optimization}
We can use direct search algos or gradient based methods
Eg: Nelder meade
Objective Function: Something we have to optimize.
Better tackles noise but too many function evaluations to achieve convergence.

WHat about COYBLA?

Gradient based methods more useful as quantum computers improve. analytically
calculating grad..ising hammy averaging and quantum circuits to calculate
gradient of energy...ultimate cost is # of iterations to achieve convergence
\subsection{Qiskit Implementation}
To tie everything together, we show how to implement VQE algorithm on LIH
molecule using IBM's qiskit package and using their quantum computers.

\end{document}
