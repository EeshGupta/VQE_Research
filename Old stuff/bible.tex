\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{comment}
\newcommand{\om}{\omega_n}
\title{Quantum Fourier Transform}
\author{Eesh Gupta }
\date{December 23rd 2019}
\begin{document}
\maketitle
  A critical tenet of most famous early quantum algorithms, Quantum Fourier Transform
  has a notoriety of being utterly non intuitive. Anything attached to ``Quantum''
  almost always arouses confusion among our intuitive thinkers and when attached to
  an algorithm like ``fourier transform''...God bless us all!

  In this paper, I aim to explore the Quantum Foruier Transform algorithm
  using matrices
  . In this journey, we will start at something as simple as polynomial
  multiplication and gradually lay the foundations for creating the quantum circuit
  for QFT.

  Some content may be supplemented by Umesh Vazirani's Youtube lecture on QFT circuit
  and Algorithms textbook (for FFT). The latter part of this paper is pure original
  content so beware!
  \section{Fast Fourier Transform}
    \subsection{Representing a Polynomial}
      Our quest begins with a simple question: How can we represent a polynomial?
      The more popular representation is the coefficient representation where,
      given a  set of coefficients \({a_0, a_1, a_2, ... , a_{n-1}}\), we can
      construct the polynomial
      \[P(x) = a_0(x^0) + a_1(x^1) + a_2(x^2) + ... +a_{n-1}(x^{n-1})\].
      However, there exists a rather less convenient point-value-representation
      where a (n-1)'th degree polynomial can be represented by the polynomial being
      evaluated at a set of n points
      \({(x_0, y_0), (x_1, y_1), (x_2, y_2), ... , (x_{n-1}, y_{n-1})}\) such
      that \(y_i = P(x_i)\) and all \(x_i\) are unique.

      Conversion from coefficient representation to point value
      representation of a polynomial \(P(x)\) can be summarized in 2 steps
      \begin{itemize}
        \item Choosing a set of n unique input points
        \(\{x_0, x_1, x_2, ... , x_{n-1}\}\)
        \item Evaluation of \(P(x_i)\) at each \(x_i\) in the set \(\{x_i\}\).
      \end{itemize}
      To visually organize such n evaluations, we use a nxn matrix \(X\) to be
      multiplied to a nx1 coefficient matrix \(\vec{a}\), resulting in a nx1 output matrix
      \(\vec{y}\).

      Caption: When we take the dot product of the ith row vector of \(X\) with the
      \(\vec{a}\), we are evaluating \(y_{i-1} = (P(x_{i-1})\).
      Insert Figure 1.

      Computing this matrix involves taking dot product of all n row vectors with
      the \(\vec{a}\). Each dot product involves computing n products. If
      we treat each multiplication operation as \(O(1)\) or constant time,
      then dotting each row vector with the coefficients matrix will
      require \(O(n)\) operations. Since there are \(n\) row vectors,
      the
     total running time of such matrix multiplication will be \(O(n^2)\).
      To avoid such quadratic order of growth, we will make a clever choice of
      input points that will allow us to achieve a new running time of
      \(O(n\log(n))\).
    \subsection{Complex Roots of Unity}
      Entering the world of exponentials, let's use \(e^{{2\pi i}/n} = \om\)
      to make a cheeky substitution:
      \[x_0 =\om^0 \]
      \[x_1 =\om^1\]
      \[.\]
      \[.\]
      \[x_k =\om^k\]
      \[.\]
      \[.\]
      \[x_{n-1} = \om^{n-1}\]
      This substitution allows us to utilize the following useful properties
      of complex exponentials, the proofs for which are left to the reader:
      \begin{itemize}
        \item \(\om^{n}\ = 1\)
        \item \(\om^{n/2}\ = -1\)
        \item \(\om^{n/4}\ = i\)
        \item \(\om^{k + n/2}\ = -\om^k\) (Halving Lemma)
      \end{itemize}
      Insert FIgure 2 (30.2 from Algo textbook with n=n instead of n=8 )
      Caption: \(X\) diagram to assist intuition with properties of complex roots of
      unity. Can you now explain why \(\om\) is called a complex root of unity?

      These properties will allow us to speed up our computation of matrix
      multiplication. The resulting savings make Fast Fourier Transform incredibly
      powerful and the quantum version even more so. For now, we make the
      the matrix multiplication problem more explicit following the above
      substitutions.

      Insert Figure 3( Figure 1 but with \(\omega\) instead of \(x\))

  \subsection{Evaluation}

    Evaluation begins with observing some symmetries which are integral to
    the FFT algorithm. Pay careful attention to the `cuts' we make to the
    original matrix \(X\) for visual intuition. It is also advised
    for the reader to follow through with an example (like \(n = 8\)) or
    derive the general
    results by themselves to appreciate the beauty of these symmetries.
    \begin{enumerate}
      \item Computing \(y_{i + n/2}\) from \(y_{i}\) using the halving lemma:

      \[
        y_{i} = P (\om^{i})
      \]
      \[
        y_{i+n/2} = P (\om^{i+n/2}) = P (-\om^{i})
      \]
      Thus, computing the upper half of \(\vec{y}\),
      i.e. \({y_1, ... , y_{n/2 -1}}\) and applying the negative sign
      to \(\omega_i\)'s takes care of the remaining terms
      in the lower half of \(\vec{y}\)  i.e.
      \({y_(n/2), ... , y_{n-1}}\). Hence our total computations are cut in
      half as we only have to compute the upper half of the \(\vec{y}\).
      This entails ignoring the lower half of matrix \(X\) as the row vectors
      in lower half of matrix \(X\) correspond to \(y_i\) in the lower half of
      \(\vec{y}\).

      Insert Figure 4( Output matrix's and A's lower half covered)

      \item Further dividing each \(y_i\):

      We can reduce our work even more by dividing the coefficient matrix
      \(\vec{a}\)  into
      2 halves: one containing even coefficients \(A_0\)and one containing
      odd coefficients \(A_1\).
      To illustrate:
      \[ A(\om) = a_0\om^0 + a_1\om^1 + ... +a_{n-1}\om^{n-1} \]
      Let
      \[ A_0(\om^{2}) = a_0\om^0 + a_2\om^2 + ... + a_{n-2}\om^{n-2} \]
      \[ A_1(\om^{2}) = a_1\om^0 + a_3\om^2 + ... + a_{n-1}\om^{n-2} \]
      Thus the original \(A(\om)\) reduces to
      \[ A(\om) = A_0(\om^{2}) +\om A_1(\om^{2}) \]
      Using the first symmetry, we arrive  at the following general result:
      \[ y_{i} = A(\om^{i}) = A_0(\om^{2i}) +\om^i(A_1(\om^{2i})) \]
      \[ y_{i+n/2}= A(-\om^{i}) = A_0(\om^{2i}) -\om^i(A_1(\om^{2i})) \]

      So how does such even-odd coefficients division affect matrix A? Since
      we are dividing the \(\vec{a}\) into even and odd entries, we
      must reorder the column vectors of \(X\) appropriately as well. Consequently,
      we must reorder the columns of matrix \(X\) such that even numbered columns are
      separated for odd numbered columns. (see figure 5)

      (figure 6 ---ith row vector in upper half and lower half)
      Notice that in the i'th row vector in the upper half of matrix A, we can
      factor out \(\om^i\) from the odd columns, resulting in the odd columns
      looking similar to even columns. Also, notice that in the i'th row
      in the lower half of matrix A, we can
      factor out \(-\om^i\) from the odd columns, resulting in the odd columns
      looking similar to even columns. These results directly follow from the
      equations above.

      This symmetry allows us to further cut our computations in half, this
      time by slashing off right half of matrix A. We can obtain right half
      by applying appropriate phase corrections to the left half.
      (figure 7: show right half of reordered matrix A covered and bottom
      of coefficient matrix A covered)

    \end{enumerate}
    Combining these symmetries, we obtain a matrix A with reordered column vectors,
    cut into quarters as shown in figure 8 below.

    Instead of computing n row-vector-coefficient-matrix dot products resulting
    in \(n^2\) computations, we use the following algorithm:
    \begin{enumerate}
      \item Divide the reordered matrix into quarters. *
      \item Compute only the top left quarter.
      \item Add relevant phase corrections to obtain the remaining 3 quarters
    \end{enumerate}

    After all these dissections to A, we are still left with computing the top left quarter.
    The trick now is to divide that quarter further into quarters, repeating the same
    algorithm. This gives fast fourier transform its recursive structure.

    Insert 9 showing recursive structure

    Now how does this visual intuition relate to the actual algorithm? Assume
    matrix A as a coordinate grid with 4 quadrants.

    \begin{verbatim}
      Recursive FFT(a)
        n = a.length                   //length of some coefficient matrix a
        if n==1
          return a                     //establish base case
        w_n = e^(2*pi*i/n)             //setting up the variables for for loop
        w = 1
        a_[0] = (a_0, a_2, ...., a_(n-2)) //even coefficients
        a_[1] = (a_1, a_3, ...., a_(n-1)) //odd coefficients
        y^[0] = Recursive FFT(a_[0])   //recursive calls y^[0] and y^[1] that fill
        y^[1] = Recursive FFT(a_[1])   //up quadrant 1 and 2 respectively
        for k = 0 to n/2 -1            // ``for each row in upper half of A''
          y_k = y^[0] + w*y^[1]        //row vector in upper half with phase correction
          y_(k+n/2) = y^[0] - w*y^[1]  //row vector in lower half with phase correction
          w = w*(w_n)                  //updating phase correction for next set of rows
        return y                       //returning array of row vectors in some quarter
                                       //of some larger matrix.
    \end{verbatim}

    To analyze the running time of this algorithm, one must recognize that we
    can only divide a nxn matrix into quarters \(\log n\) times. And after each
    division, we still have to apply proper phase corrections to row vectors
    in the remaining 3 quarters at each recursive call. These operations scale
    linearly with input size. Since there there are \(\log n\) recursive calls,
    each requiring on the order of \(O(n)\) operations, we can accept anaytically,
    that the resulting running time of Fast Fourier Transform will be
    \(O(n\log n)\), a linear speed-up over our initial, naive, gross quadratic
    algorithm!


    \subsection{Summary}
    The aim of FFT was to evaluate the point-value representation of some polynomial A. The
    savings stemmed from usage of exponentials as input points, which gave rise to the halving
    lemma which drastically cut computations. These savings
    were illustrated by making ``cuts'' to the original matrix containing the input
    points
    \section{Quantum Fourier Transform}
    Now, we have the tools to implement the Quantum Fourier Transform. The trick is
    to utilize the Recursive FFT algorithm and encode it into a quantum circuit.
    While the resulting circuit may look very different from the algorithm, the
    underlying transformations to matrix A remain similar in both cases.

    Insert Figure 10(n = 8 matrix )
    \subsection{Parallels to Fast Fourier Transform}
    While we were concerned with evaluating the point value representation of a
    polynomial in FFT section, in QFT, we are more concerned with transforming
    basis states from one basis to another. What does this mean? Consider matrix
    multiplication. In linear algebra, we are taught that a matrix represents
    a linear transformation that scales or rotates the initial basis vectors,
    transforming the space in some manner.

    Insert Figure 13( 3B1B version of matrix as linear transformation...maybe
    an untransformed matrix on left followed by action of matrix which outputs
    a transformed matrix on the right panel)

    If we considered matrix A from the previous section as some transformation
    matrix and the coefficient matrix as some basis state, then the \(\vec{y}\)
    would represent the transformed basis state. Hence, QFT and FFT are concerned
    with the same matrix multiplication problem, just under different contexts.
    Now, since QFT wants the input
    and outbut states to be basis states, the transformation matrix A must be
    unitary and the coefficient matrix must be normalized.

    For the extended example, we will assume that coefficient matrix represents
    the state \(\ket{110}\) which can be represented in matrix form as

    insert figure 14( 0 0 0 1 0 0 0 0 )

    In terms of the quantum circuit, the first qubit is in state \(\ket{0}\)
    while the other qubits are in state \(\ket{1}\).

    Insert figure 15 (quantum circuit with initial states labeled)

    Given the transformation matrix A in figure 10 and the coefficient matrix
    in figure 14, the matrix multiplication should be easy enough: the output
    matrix will be the 4th row vector of matrix A. In the following subsections,
    we will examine how the quantum circuit finds and constructs the 4th row
    vector from the initial state \(\ket{110}\).

    \subsection{Reverse Fast Fourier Transform}
    Quantum Fourier Transform can be generally described as following:
    \begin{enumerate}
      \item Reorder the columns of matrix into even and odd halves
      \item Add relevant phase corrections using single qubit and 2 qubit gates
      \item Zoom into top left quarter and REPEAT!
    \end{enumerate}
    This may seem as reverse of FFT as the former algorithm executes step 3 before
    step 2. Indeed, one may ask how can we add phase corrections before the
    computation of top left quarter?


    \subsection{Reordering columns}
    This step is rather assumed and there are no qubit operations that reorder
    the columns of matrix A. Regardless, one should not ignore this step.

    Insert Figure 11( Matrix with reordered columns and empty 3 qubit circuit)
    \subsection{Phase Corrections}
    Phase correction phase of quantum fourier transform ``finds'' the row
    vector corresponding to the coefficient matrix/initial state.
    Consider the following formulas from FFT:
    \[ y_{i} = A(\om^{i}) = A_0(\om^{2i}) +\om^i(A_1(\om^{2i})) \]
    \[ y_{i+n/2}= A(-\om^{i}) = A_0(\om^{2i}) -\om^i(A_1(\om^{2i})) \]
    Phase correction consists of 2 components
    \begin{enumerate}
      \item Hadamard gate to add or subtract \(A_0(\om^{2i})\) from
      \(A_1(\om^{2i})\).
      \item Phase gates to add the factor of \(\om^i\) to \(A_1(\om^{2i})\).
    \end{enumerate}
    The following sections will explain the intuition behind the use of
    hadamard and phase gates.

    This quest to find the row vector will follow more along the lines of
    binary search. How does binary search work? Given an element Target,
    binary search evaluates whether an array contains Target and returns
    its index in the array:
    \begin{enumerate}
      \item Find the midpoint of the sorted array
      \item Compare target with the middle element
      \item If target>middle element, binary search will only consider
      the upper half of the array (containing elements greater than
      middle element)
      \item If target< middle element, binary search will only consider
      the lower half of the array (containing elements lesser than
      middle element)
      \item Repeat!
    \end{enumerate}
    The logic for the QFT circuit (phase correction aspect) follows a similar approach. But the aim of
    QFT circuit is not only to find the row vector, but also to apply proper
    phase correction as it goes along doing so.

    Row-Vector-Search-And-Phase-Correction Algorithm:
    \begin{enumerate}
      \item Given:qubit least significant qubit (LSB) and k (the number of row
      vectors of the current matrix)
      \item If LSB is \(\ket{0}\), only consider the top half.
      \item If LSB is \(\ket{1}\), only consider the bottom half of matrix and
      apply phase factor of \(\om^{k/2}\).
      \item Move on to the next qubit and divide k by 2.
    \end{enumerate}

    \subsubsection{A Brief Look at Controlled Phase Gates}
    Let's contruct a sample 2 qubit circuit with a controlled phase gate.
    (Insert Figure 25: the circuit in question)

    The first qubit is called the control qubit and the second qubit is
    called the target qubit. Here is the working
    \begin{itemize}
      \item If the control qubit is \(\ket{0}\), then nothing happens to
      target qubit.
      \item If the control qubit is \(\ket{1}\), then a specified phase
      is added to the state of target qubit.
    \end{itemize}
    Here is a truth table and matrix multiplication to illustrate the above
    described logic.
    HADAMARD GATE IS NOT SIMPLY A PHASE GATE
    \subsubsection{Hadamard Gate}
    Consider the formulas again:
    \[ y_{i} = A(\om^{i}) = A_0(\om^{2i}) +\om^i(A_1(\om^{2i})) \]
    \[ y_{i+n/2}= A(-\om^{i}) = A_0(\om^{2i}) -\om^i(A_1(\om^{2i})) \]
    The former formula describes the ith row vector in the top half of A and
    the latter formula describes the i'th row vector in the bottom half of A.
    Careful Nielsen and Chuang graduates may recognize something weird yet
    insightful: If we let \(A_0(\om^{2i})\) to be state \(\ket{0}\) and \(A_1(\om^{2i})\) to
    be the state \(\ket{1}\), then the same equations can be written as
    \[ y_{i} = \ket{0} +\om^i\ket{1} \]
    \[ y_{i+n/2} = \ket{0} -\om^i\ket{1} \]
    which resembles an application of hadamard gate followed by phase gates. In fact,
    \(y_{i}\) corresponds to the resulting
    state of some qubit lsb being \(\ket{0}\) and \(y_{i+n/2}\) corresponds to the resulting state
    when the qubits' initial state is \(\ket{1}\). We can represent this by adding
    a hadamard gate to the first qubit:

    Insert (Figure 12: H gate on first qubit)

    One can also rexamine the following equations by considering \(\ket{0}\) representing
    the even columned entries of a row vector and \(\ket{1}\) representing the odd columned
    entries of a row vector.
    \[ y_{i} = \ket{0} +\om^i\ket{1} \]
    We add the even columned entries to odd columned entries if the row vector is contained
    in the upper half of matrix A. On the other hand, if the row vector in lower half, we
    subtract the odd columned entries from the even columned entries. This may be
    similar to invoking the first symmetry from section 1.3. For visual intuition,
    refer to figure 8.
    \[ y_{i+n/2} = \ket{0} -\om^i\ket{1} \]

    \subsubsection{Phase Gates}
    What about the additional phase correction on the odd columned entries?
    Note that the factor of \(\om^i\) corresponds to the row vector in the
    upper half of A. For example, the first row corresponds to i = 0 and
    hence a factor of \(\om^0\) = \(1\) is added to \(\ket{1}\). However,
    the third row vector corresponds to i = 2 and hence a factor of
    \(\om^2\) is added to \(\ket{1}\).
    So how can we ``find'' our row vector. Well, that's encoded in the
    intial qubit states. To illustrate, the state \(\ket{011}\) tells
    us that
    \begin{itemize}
      \item First Qubit is \(\ket{1}\): We are concerned with only the lower
      half of matrix A i.e. i \(\in\) [4,7]. Let's call this lower half of
      matrix A `B'.
      \item Second Qubit is \(\ket{1}\): We are concerned with only the lower
      half of B i.e. i \(\in\) [6,7]. Let's call this lower half of matrix B
      `C'.
      \item Third Qubit is \(\ket{0}\): Here we are concerned with the upper
      half of C i.e. \(i = 6\). At this point we know that our mystery row
      vector is the seventh row of matrix A.

    \end{itemize}
    (Insert Figure 18: 3 matrices showing the transformations above)

    Careful readers may also note that if we read the state backwards i.e. read
    \(\ket{011}\) as \(110\), this is simply the binary representation of 6
    so we can easily deduce from the state itself what row vector we are
    concerned with. (Note that this only implies for simple states without
    superposition)

    From this discussion, we conclude that locating a row vector requires us
    to consider the states of other qubits as well. However, QFT is not content
    by just finding the row vector; we must also add the phase correction
    \(\om^i\). We know that i depends on the states of all qubits but there is
    no easy way to reverse the string and convert between binary and decimal
    representations on a quantum circuit. So how do we find i and add the
    phase correction \(\om^i\)?

    Notice the pattern as I describe the process of adding phases in our
    example of \(\ket{011}\):
    \begin{enumerate}
      \item First Qubit is \(\ket{1}\): Add factor of \(\om^4 = -1\)
      \item Second Qubit is \(\ket{1}\): Add factor of \(\om^2 = i\)
      \item Third Qubit is \(\ket{0}\): Don't do anything (If third qubit
      was \(\ket{1}\) we would have added a factor of \(\om^1\))
    \end{enumerate}
    This results in the state
    \[ \ket{0} + \om^6\ket{1} = \ket{0} - \om^2\ket{1} \].
    Notice that at every stage, the power on \(\om\) is being divided by 2.
    And at the end, we get i = 6 as we predicted earlier. This evaluation
    is subtle but at essence, its just performing a conversion from binary
    to decimal. When it considers the first qubit, it already reverses
    the state. Since 1 is the third digit from the left and we know that
    the third digit corresponds to \(1*2^2 = 1*4 = 4\), the phase added in the
    first step is \(\om^4\). And if we skip to the third step, we see that
    the first digit from left is 0 and \(0*2^0 = 0*1 = 0\) which corresponds
    to no phase being added at that step.
    With the tools acquired from above discussion, you should be able to
    compute the phase added in the first recursive call to our state from
    section 2.1, \(\ket{110}\). Can you now explain the new additions to
    our quantum circuit below.
    (Insert figure 19: quantum circuit with full first recursive call)

    CAUTION ERROR WITH PHASE GATES AND HADAMARD GATE. NEED TO MERGE THESE
    SECTIONS AS JUST ONE SECTION: PHASE CORRECTION OR SOMETHING

    \subsubsection{Repeat}
    The above subsections take care of 1 recursive call. Recall from FFT
    algorithm that in the next recursive call, we are to zoom into the
    top left quarter:
    (Insert FIgure 20: unordered columns )

    Before reordering the columns, pay special attention to the entries of
    the matrix above. Notice that every row vector is a geometric series
    of \(\om^{2k}\) for \(0<k<=3\). If we were to construct a 2 qubit
    circuit for QFT or a 4*4 matrix for FFT, it would look something like
    this:
    (Insert FIgure 21: normal 4*4 FFT)
    Notice that , in contrast to figure 20, every row vector is a geometric
    series of \(\om^{k}\) for \(0<k<=3\). Since our algorithm is more adapted
    to the second case, we have to make a few adjustments. For example,
    our central equations in the second recursive call would be
    \[ y_{i} = A(\om^{2i}) = A_0(\om^{4i}) +\om^{2i}(A_1(\om^{4i})) \]
    \[ y_{i+n/2}= A(-\om^{2i}) = A_0(\om^{4i}) -\om^{2i}(A_1(\om^{4i})) \]
    as opposed to
    \[ y_{i} = A(\om^{i}) = A_0(\om^{2i}) +\om^i(A_1(\om^{2i})) \]
    \[ y_{i+n/2}= A(-\om^{i}) = A_0(\om^{2i}) -\om^i(A_1(\om^{2i})) \]
    Why does this make sense? Note that the latter set of equations represent
    the row vectors of the first recursive call. To evaluate these equations,
    one needs to evaluate \(A_0(\om^{2i})\) and \(A_1(\om^{2i})\).
    In the second recursive call, the evaluation of those terms would require
    us to replace \(\om^{i}\) with \(\om^{2i}\) in the latter set of equations.
    Hence we would obtain the first set of equations.
    Now how would we proceed with computations of the second recursive call?
    Well it would be exactly as we did with the first recursive call although
    we only work with second and third qubits now:
    \begin{enumerate}
      \item Second Qubit is \(\ket{1}\): Add factor of \((\om^2)^2 = \om^4 = -1\)
      \item Third Qubit is \(\ket{0}\): Don't do anything (If third qubit
      was \(\ket{1}\) we would have added a factor of \((\om^2)^1 = \om^2 = i\))
    \end{enumerate}
    (Insert figure 22: Show circuit)
    \subsubsection{Last Repeat}
    Third recursive call and we are only left with the third qubit. Let's look
    at the top left quarter of the reordered matrix from the second recursive
    call.
    (Insert figure 23: Show top left quarter)
    Notice how now we are dealing with series of \(\om^{4k}\) for \(0<k<=1\)?
    At this point the pattern should be apparent to you:
    \begin{enumerate}
      \item Third Qubit is \(\ket{0}\): Don't do anything (If third qubit
      was \(\ket{1}\) we would have added a factor of \((\om^4)^1 = \om^4 = -1\))
    \end{enumerate}

    (Insert Figure 24: Show final circuit)

    \subsection{Conclusion}
    It took us 10 pages and 25 figures to finally understand the QFT circuit. I
    hope it is more apparent to you why the QFT circuits look the way they do
    with their staircase like pattern. It is my understanding that Quantum
    Computing looks remarkably beautiful from this linear algebra POV than
    pure math equations. But at the same time, one needs to sift through the
    unpleasant parts of mathematics to really value the beauty of this approach.
    I thank you for going through this journey with me.



%Junk Pile alert
   \iffalse
     In the previous
    paragraph, we considered \(\ket{0}\) and \(\ket{1}\) representing the entries of a row
    vector. But before this ``pseudo-substitution'', we assumed that the first qubit being
    initially \(\ket{0}\) corresponds to the row vector being in the top half and it being
    \(\ket{1}\) corresponds to the lower half of matrix A.

    Let's carry this analogy forward: a qubit being \(\ket{0}\) corresponds to top half
    and a qubit being \(\ket{1}\) corresponds to bottom half. With the hadamard
    gate, we have already taken care of the first cut: since first qubit is \(\ket{0}\),
    our attention is on the top half of reordered matrix A, let's call it B.

    (Insert Figure 15, 16 with B first showing regular...second showing how
    each entry in lower half is simply scaled by some factor \(\om^{2}\)..
    make captions so that we are using the properties of complex root
    of unity more explicit).

    Notice that in figure 16, among the odd columned entries
    the lower half of B is similar to upper half of A, just scaled by a factor
    of \(\om^{2}\). (Ignore the -1 factor for now!) So if second qubit is
    \(\ket{0}\), we are in top half and we do nothing. If the second qubit is
    \(\ket{1}\), as in our illustration, our attention is on the bottom half and we must apply a phase
    of \(\om^{2}\) to the odd columned entries. This is achieved by applying
    a controlled phase gate in the circuit:

    (inert figure 17 with phase gate applied)

    Continuing on with the same strategy, lets zoom into the bottom half of matrix
    B and call it C.

    Let's say the first qubit is indeed \(\ket{0}\) and we are in the top half of matrix A.





    The aim of QFT is to convert states in some basis to another basis (if you've studies
    harmonic motion, the new basis might be the frequency domain). Suppose the states are
    different polynomials where each qubit is a one of the coefficients of some unique
    polynomial A. If we are to believe this annalogy, then matrix A is represented by the
    the circuit gates. The whole point of the gates is to evaluate some polynomial represented
    by the states using a given set of input points. What's constant is the input points.
    Now consider the formulas from FFT section:
    \[ y_{i} = A(\om^{i}) = A_0(\om^{2i}) +\om^i(A_1(\om^{2i})) \]
    \[ y_{i+n/2}= A(-\om^{i}) = A_0(\om^{2i}) -\om^i(A_1(\om^{2i})) \]
    If we let \(A_0(\om^{2i})\) to be state \(\ket{0}\) and \(A_1(\om^{2i})\) to
    be the state \(\ket{1}\). Then the same equations can be written as
    \[ y_{i} = \ket{0} +\om^i\ket{1} \]
    \[ y_{i+n/2} = \ket{0} -\om^i\ket{1} \]
    Careful readers may recognize this as simply the application of a hadamard gate followed
    by phase gates. If there is some qubit lsb such that \(y_{i}\) responds to the resulting
    state of input state \(\ket{0}\) and \(y_{i+n/2}\) corresponds to the resulting state
    when input state is \(\ket{1}\), we can construct the following circuit

    Matrix Intuition: When lsb is 0, we focus on the top half of matrix, slash
    the bottom half and then switch on to the second lsb and repeat algo again.
    The only drawback of this approach is that it does not explain how the phase
    gates are applied to the qubits.

    So what does each qubit encode? If we had designed this circuit classically, each qubit will encode
    the coefficients but in quantum case, each qubit is encoding a part of a matrix.

    Understanding complexities of phase: The phase gates on the first qubit are applied
    to guide the phase corrections in the original matrix with 4 quarters,
    then we expan the second qubit and apply phase correction but with a phase
    factor that's squared because n is halved. Then we zoom into a quarter and apply
    further
    \fi

  % you can't read comments. we're invisible. .-.
 %single $ $ border an inline equation, double $$x^2_{eqn}$$ border a centred equation on a new line

\end{document}
